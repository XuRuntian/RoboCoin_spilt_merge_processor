import os
import json
import argparse
import shutil
from pathlib import Path

# === é…ç½®åŒºåŸŸ ===
# ç›®æ ‡æ ‡å‡†åç§° (LeRobot v3.0 é»˜è®¤æœŸæœ›çš„åç§°)
TARGET_HIGH_NAME = "observation.images.cam_high_rgb"

# è§¦å‘å…³é”®è¯ (åªè¦ç°æœ‰ç›¸æœºååŒ…å«è¿™äº›è¯ï¼Œä¸”ä¸å« cam_high_rgbï¼Œå°±ä¼šè¢«é‡å‘½å)
KEYWORDS = ["head", "front", "font"] 

# (æ—§é€»è¾‘é…ç½®ï¼Œç›®å‰å·²å¤±æ•ˆï¼Œä¸å†æ ¹æ®æ­¤åå•åˆ é™¤æ•°æ®)
ALLOWED_CAMERAS = {
    "observation.images.cam_high_rgb",
    "observation.images.cam_left_wrist_rgb",
    "observation.images.cam_right_wrist_rgb"
}
# =================

def update_jsonl_stats(file_path, info_rename_map, dry_run=False):
    """
    åŠŸèƒ½ï¼šé€è¡Œä¿®å¤ episodes_stats.jsonl
    é€»è¾‘ï¼š1. åªé‡å‘½å  2. ä¿ç•™æ‰€æœ‰æ•°æ® (ä¸å†åˆ é™¤å¤šä½™ç›¸æœº)
    """
    if not file_path.exists():
        print(f"    âš ï¸ æœªæ‰¾åˆ°ç»Ÿè®¡æ–‡ä»¶: {file_path.name}")
        return

    print(f"    ğŸ” æ­£åœ¨æ‰«æå¹¶é‡å‘½åç»Ÿè®¡æ–‡ä»¶: {file_path.name}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"      å…±è¯»å– {len(lines)} è¡Œæ•°æ®")

    new_lines = []
    file_modified = False
    modified_count = 0

    for line in lines:
        line = line.strip()
        if not line: continue

        try:
            data = json.loads(line)
        except json.JSONDecodeError:
            new_lines.append(line)
            continue
            
        stats = data.get("stats", {})
        if not stats:
            new_lines.append(line)
            continue

        new_stats = {}
        row_modified = False
        
        # éå†è¿™ä¸€è¡Œçš„æ‰€æœ‰ç»Ÿè®¡é¡¹
        for key, value in stats.items():
            current_key = key
            
            # --- æ­¥éª¤ 1: é‡å‘½åé€»è¾‘ ---
            # ä¼˜å…ˆä½¿ç”¨ info.json çš„æ˜ å°„
            if current_key in info_rename_map:
                current_key = info_rename_map[current_key]
            # å…¶æ¬¡æ£€æŸ¥å…³é”®è¯
            elif current_key != TARGET_HIGH_NAME and current_key.startswith("observation.images."):
                is_bad_name = any(kw in current_key.lower() for kw in KEYWORDS)
                if is_bad_name:
                    current_key = TARGET_HIGH_NAME
            
            # æ£€æµ‹æ˜¯å¦å‘ç”Ÿäº†æ”¹å
            if current_key != key:
                row_modified = True

            # --- æ­¥éª¤ 2: ç›´æ¥èµ‹å€¼ (ä¸å†è¿›è¡Œç™½åå•è¿‡æ»¤) ---
            new_stats[current_key] = value
        
        if row_modified:
            data["stats"] = new_stats
            # ä½¿ç”¨ separators ç”Ÿæˆç´§å‡‘çš„ JSON
            new_lines.append(json.dumps(data, separators=(',', ':'))) 
            file_modified = True
            modified_count += 1
        else:
            new_lines.append(line)

    # å†™å…¥æ–‡ä»¶
    if file_modified:
        if dry_run:
            print(f"    [Dry Run] æ‹Ÿæ›´æ–° {modified_count} è¡Œ (ä»…é‡å‘½å)")
        else:
            with open(file_path, 'w', encoding='utf-8') as f:
                for line in new_lines:
                    f.write(line + '\n')
            print(f"    ğŸ“ {file_path.name} å·²ä¿®å¤ (æ›´æ–°äº† {modified_count} è¡Œ)")
    else:
        print(f"    âœ… {file_path.name} å†…å®¹æ— éœ€ä¿®æ”¹")

def process_single_dataset(dataset_path, dry_run=False):
    """
    æ ¸å¿ƒé€»è¾‘ï¼šå¤„ç†å•ä¸ªæ•°æ®é›† (Info Rename + Stats Rename + Video Rename)
    """
    dataset_path = Path(dataset_path)
    info_path = dataset_path / "meta/info.json"
    stats_jsonl_path = dataset_path / "meta/episodes_stats.jsonl"
    
    if not info_path.exists():
        return False, f"è·³è¿‡ (æ—  meta/info.json): {dataset_path.name}"

    print(f"\n>>> æ­£åœ¨æ‰«æ: {dataset_path.name}")

    # --- 1. è¯»å– info.json ---
    try:
        with open(info_path, 'r', encoding='utf-8') as f:
            info = json.load(f)
    except json.JSONDecodeError:
        return False, f"JSON è§£æå¤±è´¥: {info_path}"

    features = info.get("features", {})
    rename_map = {}
    
    # --- 2. æ„å»ºé‡å‘½åæ˜ å°„ (åŸºäº info.json) ---
    for key in features.keys():
        if not key.startswith("observation.images."): continue
        if key == TARGET_HIGH_NAME: continue

        lower_key = key.lower()
        for kw in KEYWORDS:
            if kw in lower_key:
                print(f"    ğŸ¯ info.json å‘ç°ç›®æ ‡: '{key}' -> æ ‡è®°ä¸º '{TARGET_HIGH_NAME}'")
                rename_map[key] = TARGET_HIGH_NAME
                break

    # --- 3. æ‰§è¡Œä¿®æ”¹ (info.json) ---
    # ä»…é‡å‘½åï¼Œä¸å†åˆ é™¤æœªåœ¨ç™½åå•çš„ features
    info_modified = False
    new_features = {}
    
    for key, value in features.items():
        # 3.1 è·å–æœ€ç»ˆåç§°
        final_key = rename_map.get(key, key)
        
        # 3.2 ç›´æ¥ä¿ç•™ (ä¸åšè¿‡æ»¤)
        new_features[final_key] = value

        if final_key != key:
            info_modified = True

    # å¦‚æœæœ‰ info.json å…¨å±€ statsï¼Œä¹Ÿåªé‡å‘½åä¸æ¸…ç†
    if "stats" in info:
        new_global_stats = {}
        for key, value in info["stats"].items():
            final_key = rename_map.get(key, key)
            new_global_stats[final_key] = value
            if final_key != key:
                info_modified = True # æ ‡è®°éœ€è¦ä¿å­˜
        info["stats"] = new_global_stats

    info["features"] = new_features

    # ä¿å­˜ info.json
    if info_modified:
        if dry_run:
            print(f"    [Dry Run] æ‹Ÿæ›´æ–° info.json (ä»…é‡å‘½å key)")
        else:
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(info, f, indent=4)
            print("    ğŸ“ info.json å·²æ›´æ–° (é‡å‘½åå®Œæˆ)")
    else:
        print("    âœ… info.json æ— éœ€ä¿®æ”¹")

    # --- 4. æ‰§è¡Œä¿®æ”¹ (episodes_stats.jsonl) --- 
    # è¿™é‡Œä¸å†è¿‡æ»¤ï¼Œåªæ”¹å
    update_jsonl_stats(stats_jsonl_path, rename_map, dry_run)

    # --- 5. æ‰§è¡Œä¿®æ”¹ (è§†é¢‘æ–‡ä»¶å¤¹) ---
    if rename_map:
        videos_root = dataset_path / "videos"
        if videos_root.exists():
            for chunk_dir in videos_root.iterdir():
                if not chunk_dir.is_dir(): continue
                
                for old_name, new_name in rename_map.items():
                    old_video_dir = chunk_dir / old_name
                    new_video_dir = chunk_dir / new_name
                    
                    if old_video_dir.exists():
                        if dry_run:
                            print(f"    [Dry Run] æ‹Ÿé‡å‘½åæ–‡ä»¶å¤¹: {old_name} -> {new_name}")
                            continue

                        if new_video_dir.exists():
                            # å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼ŒæŠŠæ–‡ä»¶ç§»è¿‡å»
                            for item in old_video_dir.iterdir():
                                try:
                                    shutil.move(str(item), str(new_video_dir / item.name))
                                except Exception:
                                    pass # å¿½ç•¥ç§»åŠ¨é”™è¯¯
                            try:
                                old_video_dir.rmdir()
                            except:
                                pass
                        else:
                            try:
                                old_video_dir.rename(new_video_dir)
                                print(f"    âœ¨ æ–‡ä»¶å¤¹é‡å‘½åæˆåŠŸ: {old_name} -> {new_name}")
                            except OSError:
                                pass

    return True, "Success"

def auto_detect_and_run(input_path, dry_run=False):
    root = Path(input_path)
    if not root.exists():
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    if (root / "meta" / "info.json").exists():
        print(f"ğŸ¤– æ¨¡å¼: å•æ•°æ®é›†å¤„ç†")
        process_single_dataset(root, dry_run)
    else:
        print(f"ğŸ¤– æ¨¡å¼: æ‰¹é‡æ ¹ç›®å½•æ‰«æ")
        subdirs = [x for x in root.iterdir() if x.is_dir()]
        count = 0
        for subdir in subdirs:
            if (subdir / "meta" / "info.json").exists():
                process_single_dataset(subdir, dry_run)
                count += 1
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼Œå…±æ‰«ææœ‰æ•ˆæ•°æ®é›†: {count} ä¸ª")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨ä¿®å¤ç›¸æœºåç§° (ä¸åˆ é™¤ä»»ä½•æ•°æ®)")
    parser.add_argument("--input", required=True, help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹æ‰§è¡Œï¼šä»…é‡å‘½åç›¸æœº...")
    auto_detect_and_run(args.input, args.dry_run)