import json
import os
import numpy as np
import pandas as pd
import argparse
from pathlib import Path

def load_json(path):
    try:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)
    except: return {}

def load_jsonl(path):
    data = []
    if not os.path.exists(path): return data
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try: data.append(json.loads(line))
                except: pass
    return data

def get_safe_indices(names_list):
    """
    æ™ºèƒ½è¿‡æ»¤é€»è¾‘ï¼šæ¨¡æ‹Ÿåˆå¹¶å·¥å…·çš„è¡Œä¸ºï¼Œè¯†åˆ«å“ªäº›ç»´åº¦æ˜¯EEFç›¸å…³åº”è¯¥è¢«å¿½ç•¥çš„
    """
    if not names_list: return [], []
    indices = []
    kept_names = []
    for i, name in enumerate(names_list):
        n = name.lower()
        # å…³é”®è¯å¿…é¡»ä¸ split_merge_dataset.py ä¸­çš„è¿‡æ»¤é€»è¾‘ä¸€è‡´
        if "end_pos" in n or "end_quat" in n or "eef_pose" in n or "robot_pos" in n or "robot_quat" in n:
            continue
        indices.append(i)
        kept_names.append(name)
    return indices, kept_names

def check_metadata(sorted_sources, merged_path):
    print(f"\n[1/3] æ­£åœ¨æ£€æŸ¥å…ƒæ•°æ® (info.json)...")
    total_eps_src = 0
    total_frames_src = 0
    for src in sorted_sources:
        info = load_json(os.path.join(src, "meta", "info.json"))
        total_eps_src += info.get('total_episodes', 0)
        total_frames_src += info.get('total_frames', 0)
        
    merged_info = load_json(os.path.join(merged_path, "meta", "info.json"))
    merged_eps = merged_info.get('total_episodes', 0)
    merged_frames = merged_info.get('total_frames', 0)
    
    print(f"  - é¢„æœŸ (æºæ•°æ®ç´¯åŠ ): {total_eps_src} eps, {total_frames_src} frames")
    print(f"  - å®é™… (åˆå¹¶ç»“æœ):   {merged_eps} eps, {merged_frames} frames")

    if merged_eps == total_eps_src and merged_frames == total_frames_src:
        print("  âœ… å…ƒæ•°æ®ç»Ÿè®¡åŒ¹é…æˆåŠŸï¼")
        return True
    else:
        print(f"  âŒ å…ƒæ•°æ®ä¸åŒ¹é…ï¼")
        return False

def check_structure(sorted_sources, merged_path):
    print(f"\n[2/3] æ­£åœ¨æ£€æŸ¥ Episode é¡ºåºä¸é•¿åº¦ (episodes.jsonl)...")
    merged_episodes = load_jsonl(os.path.join(merged_path, "meta", "episodes.jsonl"))
    merged_ep_map = {ep['episode_index']: ep for ep in merged_episodes}
    
    current_global_index = 0
    all_match = True
    
    for src in sorted_sources:
        src_episodes = load_jsonl(os.path.join(src, "meta", "episodes.jsonl"))
        for src_ep in src_episodes:
            if current_global_index not in merged_ep_map:
                all_match = False; break
            merged_ep = merged_ep_map[current_global_index]
            if src_ep.get('length') != merged_ep.get('length'):
                print(f"    âŒ é•¿åº¦ä¸ä¸€è‡´! Idx: {current_global_index}")
                all_match = False
            current_global_index += 1
            
    if all_match: print(f"  âœ… ç»“æ„éªŒè¯é€šè¿‡ ({current_global_index} æ¡æ•°æ®)")
    return all_match

def check_deep_content_smart(sorted_sources, merged_path, max_dim=None):
    print("\n[3/3] æ­£åœ¨è¿›è¡Œæ·±åº¦å†…å®¹æ¯”å¯¹ (æ•°å€¼ç²¾åº¦ä¸æ™ºèƒ½å¯¹é½)...")
    src_root = sorted_sources[0]
    
    # 1. è·å–æºæ•°æ®çš„åˆ—åå®šä¹‰
    info_path = os.path.join(src_root, "meta", "info.json")
    if not os.path.exists(info_path):
        print("  âš ï¸ æ— æ³•æ‰¾åˆ°æºæ•°æ®çš„ info.jsonï¼Œè·³è¿‡æ™ºèƒ½æ¯”å¯¹")
        return

    src_info = load_json(info_path)
    state_feat = src_info.get('features', {}).get('observation.state', {})
    names = state_feat.get('names', [])
    
    keep_indices, kept_names = get_safe_indices(names)
    print(f"  â„¹ï¸  æºç»´åº¦ {len(names)} -> æ™ºèƒ½ä¿ç•™ {len(keep_indices)} (å·²è‡ªåŠ¨å‰”é™¤ EEF/Robot Pose)")

    # 2. è¯»å–æ–‡ä»¶æ¯”å¯¹
    src_files = list(Path(src_root).rglob("episode_000000.parquet"))
    if not src_files: return
    src_file = src_files[0]
    merged_file = Path(merged_path) / "data/chunk-000/episode_000000.parquet"
    
    if not merged_file.exists():
        print(f"  âŒ æ‰¾ä¸åˆ°å¯¹åº”çš„åˆå¹¶æ–‡ä»¶: {merged_file}")
        return

    df_src = pd.read_parquet(src_file)
    df_merged = pd.read_parquet(merged_file)

    if 'observation.state' not in df_merged.columns: return

    # æå–æºå‘é‡å’Œåˆå¹¶å‘é‡
    vec_src = np.array(df_src['observation.state'].iloc[0])
    vec_merged = np.array(df_merged['observation.state'].iloc[0])

    # åº”ç”¨æ™ºèƒ½è¿‡æ»¤
    if keep_indices:
        vec_src_check = vec_src[keep_indices]
    else:
        vec_src_check = vec_src

    # æˆªå–åˆå¹¶æ•°æ®çš„æœ‰æ•ˆé•¿åº¦ï¼ˆå»é™¤æœ«å°¾è¡¥é›¶ï¼‰
    valid_len = len(vec_src_check)
    vec_merged_check = vec_merged[:valid_len]
    
    # --- ç»´åº¦è¡¥é›¶æ£€æŸ¥ ---
    if max_dim is not None:
        merged_dim = len(vec_merged)
        if merged_dim != max_dim:
             print(f"  âŒ ç»´åº¦é”™è¯¯: æœŸæœ› {max_dim}, å®é™… {merged_dim}")
        elif merged_dim > valid_len:
             padding = vec_merged[valid_len:]
             if np.allclose(padding, 0):
                 print(f"  âœ… ç»´åº¦è¡¥é›¶æ£€æŸ¥é€šè¿‡ (å¡«å……äº† {len(padding)} ä¸ª0)")
             else:
                 print(f"  âŒ è¡¥é›¶æ•°æ®å¼‚å¸¸ (å¡«å……éƒ¨åˆ†é0)")

    # --- æ•°å€¼è¯¯å·®æ£€æŸ¥ ---
    diff = np.abs(vec_src_check - vec_merged_check)
    max_diff = np.max(diff)

    if max_diff < 1e-5:
        print(f"  âœ… æ™ºèƒ½æ•°å€¼æ¯”å¯¹æˆåŠŸï¼(æœ€å¤§è¯¯å·®: {max_diff})")
        print(f"     å·²ç¡®è®¤æ ¸å¿ƒæ•°æ®ï¼ˆå…³èŠ‚ã€æ‰‹çˆªç­‰ï¼‰æ— æŸä¼ è¾“ã€‚")
    else:
        print(f"  âŒ æ•°å€¼éªŒè¯å¤±è´¥ï¼Œæœ€å¤§è¯¯å·®: {max_diff}")
        print(f"     æº: {vec_src_check[:5]}")
        print(f"     åˆ: {vec_merged_check[:5]}")

def main():
    parser = argparse.ArgumentParser(description="Verify merged dataset integrity")
    parser.add_argument('--sources', nargs='+', required=True, help="List of source dataset paths")
    parser.add_argument('--output', required=True, help="Merged dataset path")
    parser.add_argument('--max_dim', type=int, default=None, help="Expected max dimension (for zero-padding check)")
    args = parser.parse_args()

    if not os.path.exists(args.output):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è¾“å‡ºè·¯å¾„ {args.output}")
        exit(1)

    # ç¡®ä¿æºè·¯å¾„æ’åºä¸åˆå¹¶å·¥å…·ä¸€è‡´
    sorted_sources = sorted(list(set(args.sources)))
    print(f"éªŒè¯æºè·¯å¾„: {len(sorted_sources)} ä¸ª")
    
    # æŒ‰æ­¥éª¤æ‰§è¡ŒéªŒè¯
    step1 = check_metadata(sorted_sources, args.output)
    step2 = check_structure(sorted_sources, args.output)
    
    if step1 and step2:
           check_deep_content_smart(sorted_sources, args.output, args.max_dim)
           print("\nğŸ‰ === æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼=== ")
    else:
        print("\nâŒ === éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿— === ")
        exit(1)

if __name__ == "__main__":
    main()