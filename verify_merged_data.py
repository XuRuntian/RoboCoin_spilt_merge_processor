import json
import os
import numpy as np
import pandas as pd
import argparse
from pathlib import Path

def load_json(path):
    try:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)
    except: return {}

def load_jsonl(path):
    data = []
    if not os.path.exists(path): return data
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try: data.append(json.loads(line))
                except: pass
    return data

def get_safe_indices(names_list):
    """
    æ™ºèƒ½è¿‡æ»¤é€»è¾‘ï¼šæ¨¡æ‹Ÿåˆå¹¶å·¥å…·çš„è¡Œä¸ºï¼Œè¯†åˆ«å“ªäº›ç»´åº¦æ˜¯EEFç›¸å…³åº”è¯¥è¢«å¿½ç•¥çš„
    """
    if not names_list: return [], []
    indices = []
    kept_names = []
    for i, name in enumerate(names_list):
        n = name.lower()
        # å…³é”®è¯å¿…é¡»ä¸ split_merge_dataset.py ä¸­çš„è¿‡æ»¤é€»è¾‘ä¸€è‡´
        if "end_pos" in n or "end_quat" in n or "eef_pose" in n or "robot_pos" in n or "robot_quat" in n:
            continue
        indices.append(i)
        kept_names.append(name)
    return indices, kept_names

def check_metadata(sorted_sources, merged_path):
    print(f"\n[1/3] æ­£åœ¨æ£€æŸ¥å…ƒæ•°æ® (info.json)...")
    total_eps_src = 0
    total_frames_src = 0
    
    merged_info = load_json(os.path.join(merged_path, "meta", "info.json"))
    merged_eps = merged_info.get('total_episodes', 0)
    merged_frames = merged_info.get('total_frames', 0)
    merged_fps = merged_info.get('fps', None)

    fps_changed = False
    
    for src in sorted_sources:
        src_info = load_json(os.path.join(src, "meta", "info.json"))
        total_eps_src += src_info.get('total_episodes', 0)
        total_frames_src += src_info.get('total_frames', 0)
        
        src_fps = src_info.get('fps', merged_fps) # å¦‚æœæºæ²¡å†™FPSï¼Œå‡è®¾å’Œç›®æ ‡ä¸€è‡´
        if merged_fps is not None and src_fps is not None and abs(src_fps - merged_fps) > 1e-5:
            fps_changed = True

    print(f"  - é¢„æœŸ (æºæ•°æ®ç´¯åŠ ): {total_eps_src} eps, {total_frames_src} frames (æºFPSç´¯åŠ å€¼)")
    print(f"  - å®é™… (åˆå¹¶ç»“æœ):   {merged_eps} eps, {merged_frames} frames (FPS={merged_fps})")

    if fps_changed:
        print(f"  âš ï¸ æ£€æµ‹åˆ° FPS å‘ç”Ÿå˜åŒ–ï¼Œæ€»å¸§æ•°å¿…ç„¶ä¸åŒã€‚è·³è¿‡æ€»å¸§æ•°ä¸¥æ ¼æ¯”å¯¹ï¼Œå°†åœ¨æ­¥éª¤[2/3]ä¸­é€ä¸ªéªŒè¯é‡é‡‡æ ·é•¿åº¦ã€‚")
        if merged_eps == total_eps_src:
            print("  âœ… Episode æ•°é‡åŒ¹é…æˆåŠŸï¼")
            return True
        else:
            print(f"  âŒ Episode æ•°é‡ä¸åŒ¹é…ï¼(é¢„æœŸ {total_eps_src}, å®é™… {merged_eps})")
            return False
    else:
        if merged_eps == total_eps_src and merged_frames == total_frames_src:
            print("  âœ… å…ƒæ•°æ®ç»Ÿè®¡åŒ¹é…æˆåŠŸï¼")
            return True
        else:
            print(f"  âŒ å…ƒæ•°æ®ä¸åŒ¹é…ï¼")
            return False

def check_structure(sorted_sources, merged_path):
    print(f"\n[2/3] æ­£åœ¨æ£€æŸ¥ Episode é¡ºåºä¸é•¿åº¦ (episodes.jsonl)...")
    merged_episodes = load_jsonl(os.path.join(merged_path, "meta", "episodes.jsonl"))
    merged_ep_map = {ep['episode_index']: ep for ep in merged_episodes}
    merged_info = load_json(os.path.join(merged_path, "meta", "info.json"))
    merged_fps = merged_info.get('fps', 30)
    
    current_global_index = 0
    all_match = True
    
    for src in sorted_sources:
        src_episodes = load_jsonl(os.path.join(src, "meta", "episodes.jsonl"))
        src_info = load_json(os.path.join(src, "meta", "info.json"))
        src_fps = src_info.get('fps', merged_fps) # é»˜è®¤å‡è®¾ä¸€è‡´

        for src_ep in src_episodes:
            if current_global_index not in merged_ep_map:
                print(f"    âŒ ç¼ºå°‘ç´¢å¼•: {current_global_index}")
                all_match = False; break
            
            merged_ep = merged_ep_map[current_global_index]
            
            # === è®¡ç®—é¢„æœŸé•¿åº¦ (åŒ…å«é‡é‡‡æ ·é€»è¾‘) ===
            expected_len = src_ep.get('length')
            if merged_fps and src_fps and abs(merged_fps - src_fps) > 1e-5:
                # å¤åˆ¶ RoboCOIN_dataset_lib.py ä¸­çš„é‡é‡‡æ ·é€»è¾‘
                duration = expected_len / src_fps
                expected_len = int(np.ceil(duration * merged_fps))
            
            if expected_len != merged_ep.get('length'):
                print(f"    âŒ é•¿åº¦ä¸ä¸€è‡´! Idx: {current_global_index} (æº: {src_ep.get('length')}@{src_fps}fps -> é¢„æœŸ: {expected_len} -> å®é™…: {merged_ep.get('length')}@{merged_fps}fps)")
                all_match = False
            
            current_global_index += 1
            
    if all_match: print(f"  âœ… ç»“æ„éªŒè¯é€šè¿‡ ({current_global_index} æ¡æ•°æ®)")
    return all_match

def check_deep_content_smart(sorted_sources, merged_path, max_dim=None):
    print("\n[3/3] æ­£åœ¨è¿›è¡Œæ·±åº¦å†…å®¹æ¯”å¯¹ (æ•°å€¼ç²¾åº¦ä¸æ™ºèƒ½å¯¹é½)...")
    src_root = sorted_sources[0]
    
    # 1. è·å–æºæ•°æ®çš„åˆ—åå®šä¹‰
    info_path = os.path.join(src_root, "meta", "info.json")
    if not os.path.exists(info_path):
        print("  âš ï¸ æ— æ³•æ‰¾åˆ°æºæ•°æ®çš„ info.jsonï¼Œè·³è¿‡æ™ºèƒ½æ¯”å¯¹")
        return

    src_info = load_json(info_path)
    state_feat = src_info.get('features', {}).get('observation.state', {})
    names = state_feat.get('names', [])
    
    keep_indices, kept_names = get_safe_indices(names)
    print(f"  â„¹ï¸  æºç»´åº¦ {len(names)} -> æ™ºèƒ½ä¿ç•™ {len(keep_indices)} (å·²è‡ªåŠ¨å‰”é™¤ EEF/Robot Pose)")

    # 2. è¯»å–æ–‡ä»¶æ¯”å¯¹
    # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª parquet æ–‡ä»¶
    src_file = None
    merged_file = None
    
    # æŸ¥æ‰¾æºæ–‡ä»¶ (éå†ç›´åˆ°æ‰¾åˆ°å­˜åœ¨çš„ episode_000000)
    for root, _, files in os.walk(os.path.join(src_root)):
        for f in files:
            if "episode_000000.parquet" in f:
                src_file = os.path.join(root, f)
                break
        if src_file: break

    # æŸ¥æ‰¾ç›®æ ‡æ–‡ä»¶
    merged_file = os.path.join(merged_path, "data/chunk-000/episode_000000.parquet")
    
    if not src_file or not os.path.exists(src_file):
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æºæ–‡ä»¶ episode_000000.parquetï¼Œè·³è¿‡å†…å®¹æ¯”å¯¹")
        return
    
    if not os.path.exists(merged_file):
        print(f"  âŒ æ‰¾ä¸åˆ°å¯¹åº”çš„åˆå¹¶æ–‡ä»¶: {merged_file}")
        return

    try:
        df_src = pd.read_parquet(src_file)
        df_merged = pd.read_parquet(merged_file)
    except Exception as e:
        print(f"  âŒ è¯»å– Parquet å¤±è´¥: {e}")
        return

    if 'observation.state' not in df_merged.columns: return

    # æå–æºå‘é‡å’Œåˆå¹¶å‘é‡
    try:
        vec_src = np.array(df_src['observation.state'].iloc[0])
        vec_merged = np.array(df_merged['observation.state'].iloc[0])
    except Exception as e:
        print(f"  âŒ æ•°æ®æå–å¤±è´¥: {e}")
        return

    # åº”ç”¨æ™ºèƒ½è¿‡æ»¤
    if keep_indices:
        # ç¡®ä¿ç´¢å¼•ä¸è¶…è¿‡æºå‘é‡é•¿åº¦ (é˜²æ­¢æºæ•°æ®å·²ç»æ›´çŸ­)
        valid_indices = [i for i in keep_indices if i < len(vec_src)]
        vec_src_check = vec_src[valid_indices]
    else:
        vec_src_check = vec_src

    # æˆªå–åˆå¹¶æ•°æ®çš„æœ‰æ•ˆé•¿åº¦ï¼ˆå»é™¤æœ«å°¾è¡¥é›¶ï¼‰
    valid_len = len(vec_src_check)
    if valid_len > len(vec_merged):
        print(f"  âŒ ç»´åº¦å¼‚å¸¸: æºæœ‰æ•ˆç»´åº¦ {valid_len} > åˆå¹¶ç»´åº¦ {len(vec_merged)}")
        return
        
    vec_merged_check = vec_merged[:valid_len]
    
    # --- ç»´åº¦è¡¥é›¶æ£€æŸ¥ ---
    if max_dim is not None:
        merged_dim = len(vec_merged)
        if merged_dim != max_dim:
             print(f"  âŒ ç»´åº¦é”™è¯¯: æœŸæœ› {max_dim}, å®é™… {merged_dim}")
        elif merged_dim > valid_len:
             padding = vec_merged[valid_len:]
             if np.allclose(padding, 0):
                 print(f"  âœ… ç»´åº¦è¡¥é›¶æ£€æŸ¥é€šè¿‡ (å¡«å……äº† {len(padding)} ä¸ª0)")
             else:
                 print(f"  âŒ è¡¥é›¶æ•°æ®å¼‚å¸¸ (å¡«å……éƒ¨åˆ†é0)")

    # --- æ•°å€¼è¯¯å·®æ£€æŸ¥ ---
    try:
        diff = np.abs(vec_src_check - vec_merged_check)
        max_diff = np.max(diff)

        if max_diff < 1e-4: # ç¨å¾®æ”¾å®½ä¸€ç‚¹ï¼Œè€ƒè™‘åˆ°æµ®ç‚¹è½¬æ¢
            print(f"  âœ… æ™ºèƒ½æ•°å€¼æ¯”å¯¹æˆåŠŸï¼(æœ€å¤§è¯¯å·®: {max_diff:.6f})")
            print(f"     å·²ç¡®è®¤æ ¸å¿ƒæ•°æ®ï¼ˆå…³èŠ‚ã€æ‰‹çˆªç­‰ï¼‰æ— æŸä¼ è¾“ã€‚")
        else:
            print(f"  âŒ æ•°å€¼éªŒè¯å¤±è´¥ï¼Œæœ€å¤§è¯¯å·®: {max_diff}")
            print(f"     æº: {vec_src_check[:5]}")
            print(f"     åˆ: {vec_merged_check[:5]}")
    except ValueError as e:
        print(f"  âŒ å½¢çŠ¶ä¸åŒ¹é…æ— æ³•æ¯”å¯¹: {e}")

def main():
    parser = argparse.ArgumentParser(description="Verify merged dataset integrity")
    parser.add_argument('--sources', nargs='+', required=True, help="List of source dataset paths")
    parser.add_argument('--output', required=True, help="Merged dataset path")
    parser.add_argument('--max_dim', type=int, default=None, help="Expected max dimension (for zero-padding check)")
    args = parser.parse_args()

    if not os.path.exists(args.output):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è¾“å‡ºè·¯å¾„ {args.output}")
        exit(1)

    # ç¡®ä¿æºè·¯å¾„æ’åºä¸åˆå¹¶å·¥å…·ä¸€è‡´
    sorted_sources = sorted(list(set(args.sources)))
    print(f"éªŒè¯æºè·¯å¾„: {len(sorted_sources)} ä¸ª")
    
    # æŒ‰æ­¥éª¤æ‰§è¡ŒéªŒè¯
    step1 = check_metadata(sorted_sources, args.output)
    step2 = check_structure(sorted_sources, args.output)
    
    if step1 and step2:
           check_deep_content_smart(sorted_sources, args.output, args.max_dim)
           print("\nğŸ‰ === æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼=== ")
    else:
        print("\nâŒ === éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿— === ")
        exit(1)

if __name__ == "__main__":
    main()